from __future__ import annotations
import sys
from pathlib import Path
import time
from typing import Optional, List
import json

import streamlit as st

# Ensure repository root is importable when run via Streamlit
_THIS_FILE = Path(__file__).resolve()
_REPO_ROOT = _THIS_FILE.parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from src.lib.image_scraper import scrape_images
from src.lib.topic_discovery import discover_topic, download_selected, filter_entries
from src.lib.models_discovery import DownloadFilter, ProvenanceEntry
from src.lib import image_scraper as scraper


st.set_page_config(page_title="image-saver | ÁîªÂÉè„Çπ„ÇØ„É¨„Ç§„Éë„Éº", layout="centered")

st.title("URL/„Éà„Éî„ÉÉ„ÇØ„ÅßÁîªÂÉè„Çπ„ÇØ„É¨„Ç§„Éë„Éº")
st.caption("URL„Éó„É¨„Éì„É•„Éº ‚Üí ÈÅ∏Êäû„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÄÅ„Åæ„Åü„ÅØ„Éà„Éî„ÉÉ„ÇØÔºàÊ§úÁ¥¢„ÉØ„Éº„ÉâÔºâ„Åã„ÇâËá™ÂæãÊé¢Á¥¢„Éó„É¨„Éì„É•„Éº„ÄÇrobots.txt „ÇíÂ∞äÈáç„Åó„Åæ„Åô„ÄÇ")

# --- Input Section ---
url = st.text_input("ÂØæË±°URL", placeholder="https://example.com")
topic = st.text_input("„Éà„Éî„ÉÉ„ÇØÔºàÊ§úÁ¥¢„ÉØ„Éº„ÉâÔºâ", placeholder="‰æã: ÂØåÂ£´Â±± Á¥ÖËëâ", help="URL„ÅÆ‰ª£„Çè„Çä„Å´„Éà„Éî„ÉÉ„ÇØ„ÇíÂÖ•Âäõ„Åó„Å¶„Éó„É¨„Éì„É•„Éº")
output_dir = st.text_input("‰øùÂ≠òÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™", value="./images")
limit = st.number_input("ÊúÄÂ§ßÊûöÊï∞(‰ªªÊÑè)", min_value=0, max_value=500, value=0, help="0„ÅØ‰∏äÈôê„Å™„Åó")
respect_robots = st.toggle("robots.txt „ÇíÂ∞äÈáç", value=True)

# --- US2: Filter Section ---
with st.expander("üîç „Éï„Ç£„É´„Çø„ÉºË®≠ÂÆö (US2)", expanded=False):
    st.caption("„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊôÇ„Å´ÈÅ©Áî®„Åï„Çå„Çã„Éï„Ç£„É´„Çø„Éº")
    col_f1, col_f2 = st.columns(2)
    with col_f1:
        min_width = st.number_input("ÊúÄÂ∞èÂπÖ (px)", min_value=0, value=0, help="0„ÅßÂà∂Èôê„Å™„Åó")
    with col_f2:
        min_height = st.number_input("ÊúÄÂ∞èÈ´ò„Åï (px)", min_value=0, value=0, help="0„ÅßÂà∂Èôê„Å™„Åó")
    allow_domains = st.text_input("Ë®±ÂèØ„Éâ„É°„Ç§„É≥ („Ç´„É≥„ÉûÂå∫Âàá„Çä)", placeholder="pixabay.com, unsplash.com", help="Á©∫Ê¨Ñ„ÅßÂÖ®„Å¶Ë®±ÂèØ")
    deny_domains = st.text_input("Èô§Â§ñ„Éâ„É°„Ç§„É≥ („Ç´„É≥„ÉûÂå∫Âàá„Çä)", placeholder="spam.com, ads.example.net", help="Á©∫Ê¨Ñ„ÅßÈô§Â§ñ„Å™„Åó")
    st.divider()
    st.caption("Èñ¢ÈÄ£Â∫¶„Éï„Ç£„É´„Çø„ÉºÔºà„Éà„Éî„ÉÉ„ÇØÊ§úÁ¥¢ÊôÇ„ÅÆ„ÅøÊúâÂäπÔºâ")
    min_relevance = st.select_slider(
        "ÊúÄÂ∞èÈñ¢ÈÄ£Â∫¶",
        options=["„Åô„Åπ„Å¶", "‰Ωé‰ª•‰∏ä", "‰∏≠‰ª•‰∏ä", "È´ò„ÅÆ„Åø"],
        value="‰Ωé‰ª•‰∏ä",
        help="„Éà„Éî„ÉÉ„ÇØÊ§úÁ¥¢ÊôÇ„ÄÅÊåáÂÆö„É¨„Éô„É´Êú™Ê∫Ä„ÅÆÁîªÂÉè„ÇíÈùûË°®Á§∫"
    )
    # Map label to threshold
    relevance_thresholds = {"„Åô„Åπ„Å¶": 0.0, "‰Ωé‰ª•‰∏ä": 0.0, "‰∏≠‰ª•‰∏ä": 0.3, "È´ò„ÅÆ„Åø": 0.6}
    min_relevance_score = relevance_thresholds[min_relevance]

search_term = st.text_input("Ê§úÁ¥¢„Éï„Ç£„É´„Çø („Éï„Ç°„Ç§„É´Âêç/URL ÈÉ®ÂàÜ‰∏ÄËá¥)", value="")
page_size = st.selectbox("„Éö„Éº„Ç∏„Çµ„Ç§„Ç∫", [10, 25, 50, 100], index=1)
select_all_toggle = st.checkbox("ÂÖ®ÈÅ∏Êäû/ÂÖ®Ëß£Èô§", value=False)

col_r1, col_r2, col_r3 = st.columns([1,1,2])
with col_r1:
    run_preview = st.button("URL„Éó„É¨„Éì„É•„Éº")
with col_r2:
    clear_sel = st.button("ÈÅ∏Êäû„Çí„ÇØ„É™„Ç¢")
col_r4 = st.container()
with col_r4:
    run_topic = st.button("„Éà„Éî„ÉÉ„ÇØ„Åß„Éó„É¨„Éì„É•„Éº")

if clear_sel:
    st.session_state.pop("preview_urls", None)
    st.session_state.pop("selected", None)
    st.session_state.pop("provenance_entries", None)

if run_preview:
    if not url.strip():
        st.error("URL„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
    else:
        with st.spinner("ÁîªÂÉèURL„ÇíÂèñÂæó‰∏≠..."):
            try:
                lim: Optional[int] = None if limit == 0 else int(limit)
                urls = scraper.list_images(url.strip(), limit=lim, respect_robots=respect_robots)
                st.session_state["preview_urls"] = urls
                st.session_state["selected"] = set()
                st.session_state["provenance_entries"] = None  # URL mode has no provenance
                st.success(f"Ê§úÂá∫: {len(urls)} Êûö„ÅÆÁîªÂÉèÂÄôË£ú (URL)")
            except PermissionError as e:
                st.warning(f"robots.txt „Å´„Çà„Çä„Éñ„É≠„ÉÉ„ÇØ„Åï„Çå„Åæ„Åó„Åü: {e}")
            except Exception as e:
                st.error(f"Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")

if run_topic:
    if not topic.strip():
        st.error("„Éà„Éî„ÉÉ„ÇØ„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
    else:
        with st.spinner("„Éà„Éî„ÉÉ„ÇØ„Åã„ÇâÊé¢Á¥¢‰∏≠..."):
            try:
                lim: Optional[int] = None if limit == 0 else int(limit)
                preview = discover_topic(topic.strip(), limit=lim or 50)
                st.session_state["preview_urls"] = [str(e.image_url) for e in preview.entries]
                st.session_state["selected"] = set()
                st.session_state["provenance_entries"] = preview.entries  # Store for US2
                st.success(f"Ê§úÂá∫: {preview.total_images} Êûö„ÅÆÁîªÂÉèÂÄôË£ú („Éà„Éî„ÉÉ„ÇØ: {topic})")
            except Exception as e:
                st.error(f"Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")

preview_urls = st.session_state.get("preview_urls", [])
selected: set[str] = st.session_state.get("selected", set())
provenance_entries: Optional[List[ProvenanceEntry]] = st.session_state.get("provenance_entries", None)

# Build URL to entry mapping for relevance display
url_to_entry: dict[str, ProvenanceEntry] = {}
if provenance_entries:
    url_to_entry = {str(e.image_url): e for e in provenance_entries}

# Apply search filter
if search_term.strip():
    filtered = [u for u in preview_urls if search_term.lower() in str(u).lower()]
else:
    filtered = preview_urls

# Apply relevance filter (topic mode only)
if provenance_entries and min_relevance_score > 0:
    filtered = [u for u in filtered if url_to_entry.get(u, None) and url_to_entry[u].relevance_score >= min_relevance_score]

# Pagination state
total = len(filtered)
page_count = max(1, (total + page_size - 1) // page_size)
page_index = st.session_state.get("page_index", 0)
col_p1, col_p2, col_p3 = st.columns([1,1,3])
with col_p1:
    if st.button("Ââç„Å∏", disabled=page_index <= 0):
        page_index = max(0, page_index - 1)
with col_p2:
    if st.button("Ê¨°„Å∏", disabled=page_index >= page_count - 1):
        page_index = min(page_count - 1, page_index + 1)
st.session_state["page_index"] = page_index
start = page_index * page_size
end = start + page_size
page_slice = filtered[start:end]

if preview_urls:
    st.subheader("„Éó„É¨„Éì„É•„ÉºÔºàÈÅ∏ÊäûÂèØËÉΩÔºâ")
    # Select all toggle
    if select_all_toggle:
        for u in page_slice:
            selected.add(str(u))
    else:
        # If toggled off, remove only those in page_slice (do not clear global manual selections on other pages)
        for u in page_slice:
            u_str = str(u)
            if u_str in selected and f"sel_{preview_urls.index(u)}" not in st.session_state:
                # keep manual boxes; rely on user unchecking for removal
                pass
    cols = st.columns(5)
    for idx_global, u in enumerate(page_slice):
        col = cols[idx_global % 5]
        u_str = str(u)
        with col:
            st.image(u_str, caption=Path(u_str).name, use_container_width=True)
            # Show relevance badge for topic mode
            entry = url_to_entry.get(u_str)
            if entry:
                score = entry.relevance_score
                label = entry.get_relevance_label()
                if score >= 0.6:
                    st.success(f"üü¢ {label} ({score:.2f})")
                elif score >= 0.3:
                    st.warning(f"üü° {label} ({score:.2f})")
                else:
                    st.error(f"üî¥ {label} ({score:.2f})")
            key = f"sel_{preview_urls.index(u)}"
            checked = st.checkbox("ÈÅ∏Êäû", key=key, value=(u_str in selected))
            if checked:
                selected.add(u_str)
            else:
                selected.discard(u_str)
    st.session_state["selected"] = selected

    col_d1, col_d2 = st.columns([1,1])
    with col_d1:
        do_download_all = st.button("ÂÖ®„Å¶„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ")
    with col_d2:
        do_download_sel = st.button("ÈÅ∏Êäû„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ")

    if do_download_all or do_download_sel:
        target_urls = [str(u) for u in preview_urls] if do_download_all else list(selected)
        if not target_urls:
            st.info("ÈÅ∏Êäû„Åï„Çå„ÅüÁîªÂÉè„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        else:
            with st.spinner("„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ‰∏≠..."):
                try:
                    progress = st.progress(0)
                    def cb(done, total):
                        progress.progress(int(done / total * 100))

                    # Build filter
                    download_filter = DownloadFilter(
                        min_width=min_width if min_width > 0 else None,
                        min_height=min_height if min_height > 0 else None,
                        allow_domains=[d.strip() for d in allow_domains.split(",") if d.strip()] if allow_domains.strip() else None,
                        deny_domains=[d.strip() for d in deny_domains.split(",") if d.strip()] if deny_domains.strip() else None,
                    )

                    # US2: If we have provenance entries (topic mode), use download_selected
                    if provenance_entries:
                        # Filter entries to only selected URLs
                        selected_entries = [e for e in provenance_entries if str(e.image_url) in target_urls]

                        paths, index_path = download_selected(
                            entries=selected_entries,
                            output_dir=output_dir.strip(),
                            download_filter=download_filter,
                            respect_robots=respect_robots,
                            progress_cb=cb,
                        )

                        st.success(f"‰øùÂ≠ò: {len(paths)} Êûö | Provenance Index: {index_path}")
                    else:
                        # URL mode: apply domain filter manually then download
                        filtered_target = target_urls
                        if download_filter.allow_domains or download_filter.deny_domains:
                            from urllib.parse import urlparse
                            def _domain_check(u: str) -> bool:
                                domain = urlparse(u).netloc.lower()
                                if download_filter.allow_domains:
                                    if not any(domain == d.lower() or domain.endswith("." + d.lower()) for d in download_filter.allow_domains):
                                        return False
                                if download_filter.deny_domains:
                                    if any(domain == d.lower() or domain.endswith("." + d.lower()) for d in download_filter.deny_domains):
                                        return False
                                return True
                            filtered_target = [u for u in target_urls if _domain_check(u)]

                        paths = scraper.download_images_parallel(filtered_target, output_dir.strip(), respect_robots=respect_robots, progress_cb=cb)
                        st.success(f"‰øùÂ≠ò: {len(paths)} Êûö")

                    if paths:
                        grid = st.columns(5)
                        for i, p in enumerate(paths[:20]):  # Show max 20 thumbnails
                            with grid[i % 5]:
                                st.image(p, caption="‚úÖ " + Path(p).name, use_container_width=True)

                        # Offer ZIP download to user
                        import io, zipfile
                        mem = io.BytesIO()
                        with zipfile.ZipFile(mem, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
                            for p in paths:
                                with open(p, "rb") as f:
                                    zf.writestr(Path(p).name, f.read())
                        mem.seek(0)
                        dl_filename = "images_download.zip"
                        st.download_button(
                            label="ZIP„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ",
                            data=mem.getvalue(),
                            file_name=dl_filename,
                            mime="application/zip"
                        )
                except Exception as e:
                    st.error(f"„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
